
## Parameter Optimization
1. *Gradient Decent</br>
   - 전체 데이터 사용
2. *Stochastic Gradient Descent (SGD)</br>
   - Gradient Decent와 metrics는 동일하지만 한번 업데이트 시 일부데이터만 사용함 (시간,메모리 단축)
   - 일부데이터, 랜덤 미니배치 (stochastic이라는 단어 붙음)
   
4. Mini-Batch Gradient Descent</br>
5. Conjugate Gradient Descent</br>
6. Quasi-Newton Methods (like BFGS)</br>


