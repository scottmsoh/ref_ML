
1. Cross-Entropy Formula
    H(p,q) = - sum of p(x) * logq(x)</br>
           = −[ylog(p)+(1−y)log(1−p)]</br>
    p is true distribution, q is predicted distribution.</br>
    y is true label, p is the predicted probability of the class with label 1.</br>

    A lower cross-entropy error indicates that the predicted distribution is closer to the true distribution</br>


   

