
1. Cross-Entropy Formula</br>
   This can be used as cost function in classification models</br>
   Definition is a method to quantify the error or cost associated with a given model's predictions compared to the actual outcomes.</br>

    H(p,q) = - sum of p(x) * logq(x)</br>
           = −[ylog(p)+(1−y)log(1−p)]</br>
    p is true distribution, q is predicted distribution.</br>
    y is true label, p is the predicted probability of the class with label 1.</br>

    A lower cross-entropy error indicates that the predicted distribution is closer to the true distribution</br>


![Image 2023-12-27 at 10 47 AM](https://github.com/scottmsoh/ref_ML/assets/112598791/06b1c5d3-c845-42c3-96f6-57edd6f07c3c)
   

