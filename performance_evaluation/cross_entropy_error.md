
1. Cross-Entropy Formula
    H(p,q) = - sum of p(x) * logq(x)
           = −[ylog(p)+(1−y)log(1−p)]
    p is true distribution, q is predicted distribution.
    y is true label, p is the predicted probability of the class with label 1.

    A lower cross-entropy error indicates that the predicted distribution is closer to the true distribution,


   

