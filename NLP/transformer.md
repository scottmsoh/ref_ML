

### Transformer model (google)

decoder and encoder (RNN, LSTM)</br>
순차적 처리로 인한 병렬처리 불가, 앞의 모델이 영향이 제대로 영향 안됨</br>

트랜스포머 모델은 self-attention mechanism을 기반으로 하는 새로운 아키텍쳐를 도입함으로써</br>
NLP를 혁신적으로 변화 시켰음</br>

특히 데이터의 전체 시퀀스를 병력적으로 처리할 수 있게 함으로써, RNN과 LSTM과 같은 이전 sequential모델의</br>
한계, 특히 long-range dependencies를 해결하고 더 효율적인 훈련을 가능하게 했습니다.</br>

트런스포머가 NLP에 미친 영향은 translation, text generation, semantic understanding등의</br>
state of art models 특히 BERT, GPT같은 모델의 기반이 됨</br>


### self-attention mechanism 과 LSTM-based seq2seq는 어떤 차이?

